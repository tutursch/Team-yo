{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9adf41",
   "metadata": {},
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" width=500></td>\n",
    "<img src=\"https://www.zigobot.ch/images/stories/virtuemart/product/Thymio_II_5288c11c8c241.jpg\" width=500>\n",
    "# <center>Project of Basics of mobile robotics</center>\n",
    "## <center> Broccard Brendan, Ferreira Lopes Filipe, Pillet Maxime, Schramm Arthur</center>\n",
    "<hr style=\"clear:both\">\n",
    "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
    "This Juypter notebook will be the report of our project as part of the course \"Basics of mobile robotics\" given by Prof. Francesco Mondada.</p>\n",
    "<hr style=\"clear:both\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c08e7",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "<p>$\\;\\;\\;\\;\\;\\;$ In this project, the goal was to create an environment in which we would have a starting point, a goal to reach and multiple obstacles in the way. We implented the following code on our Thymio robot in order to make it go from the starting point to the goal, following the most efficient path through the initial obstacles and using its sensors to avoid sudden obstacles on its way. A camera allows us to create a map of the environment at the beginning of the program and further on to detect the robot's position and orientation. That information is first filtered with a Kalman filter and then processed by a PD controller that we created in order to make Thymio go smoothly in direction of it's desired goal. Thanks to the Kalman filter, if the camera is suddenly obstruated, we can still estimate the robot's position and orientation. We update our values in a main \"while\" loop that occurs every 0.1 second.\n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ This is a typical map pattern with the thymio at the start, a local obstacle on its way, the final goal in green and the obstacles in black : \n",
    "    \n",
    "<img src=\"Images/map.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3606e03",
   "metadata": {},
   "source": [
    "## 2. Details on each section of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c2fd8",
   "metadata": {},
   "source": [
    "### 2.1 Vision\n",
    "<p>$\\;\\;\\;\\;\\;\\;$ In this section, we will explain how we implemented the vision to our project. The goal is to understand the map and get the positions of the start/stop points and the corners of our obstacles. \n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ The first step of the vision is a \"while\" loop at the beginning of our process, even before the Thymio is placed on the map. The loop is asked to detect at least the start and stop chips which is the condition to stop executing the loop. The chips are differientiated by their HSV color code, the start is blue and the finish green. It also checks if we have initial obstacles in our circuit and computes the positions of their corners. We consider the corners shifted in order to keep a security margin that considers Thymio's width, which results in mapping the obstacles wider than they really are to ensure their complete avoidance.\n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ This first vision step gives us a map with our start, stop and enlarged corners positions : \n",
    "\n",
    "<img src=\"Images/Vision_avec_legende.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ The second step is to detect the position and the facing angle of Thymio after we placed it on the start chip. It is continuously computed since it is updated in our main loop. To do so, we pasted a yellow chip in the front of Thymio and a red one in the back. The robot's position is considered as being in the middle of the two chips. The position of the yellow chip, coupled with the position of the red one, allows us to have the orientation of Thymio. \n",
    "<br>\n",
    "<br> $\\;\\;\\;\\;\\;\\;$The robot's position is given by the blue circle and its orientation is given by the green line :\n",
    "<br>\n",
    "<img src=\"Images/Thymio_orientation.png\" alt=\"Drawing\" style=\"width: 150px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef2e0c",
   "metadata": {},
   "source": [
    "### 2.2 Global Navigation\n",
    "<p>$\\;\\;\\;\\;\\;\\;$ The global navigation computes the shortest path to go from a starting point to a finish point while going through our obstacle field. We chose to use the visibility graph as the pathfinding algorithm. It consists of a web of every possible routes to go from a point A to a point B passing by the angles of obstacles. Next, it calculates the shortest euclidian route and considers it as the optimized path to take. This path is given in the form of an array of points to reach in the given order.  \n",
    "<br> \n",
    "<br>$\\;\\;\\;\\;\\;\\;$ The visibility graph, when applied to our map, returns this path :\n",
    "<img src=\"Images/VisGraph.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5a48cd",
   "metadata": {},
   "source": [
    "### 2.3 Filtering\n",
    "<p>$\\;\\;\\;\\;\\;\\;$ Since the camera's datas are uncertain (due to noise or lack of precision), we use the Kalman filter based on theses datas in order to get a good estimation of the position and orientation of the robot (which are the states of Thymio that we calculate at each time step). In order to estimate the new state, we use the last position, oriention and speed of the motors (linear and rotation). Then, Thymio's states are updated, based on the last measurements, only if the camera detects the robot. This update compares the real measurements and the estimated ones, in order to make the algorithm better (by calculating the residual covariance matrix and the Kalman gain). If the camera doesn't detect Thymio, we use the prediction the localize the robot, that allows the robot to work even though the camera is hidden. The following formula represents the state space model that we use at each time step :\n",
    "<img src=\"Images/Kalman.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ffe3a",
   "metadata": {},
   "source": [
    "### 2.4 Local Navigation\n",
    "<p>$\\;\\;\\;\\;\\;\\;$For this section, most of the code was inspired by the exercise 3 of the course given by Prof. Francesco Mondada. Indeed, in the motion_control.py file, we can find two founctions that are used for local avoidance : check_obstacle and local_avoidance. The first one just returns a boolean that tells us if any of the front sensors has a value that exceeds the trigger value in which case we consider that an obstacle has been detected. If so, the second function will use a system of weights inspired from the exercise mentionned above to return a certain speed value, that will be added to the basic speed of the robot, in order to make it turn smoothly and avoid the obstacle. There will be more info in the following section on motion control concerning how this speed value, that is returned by the function local_avoidance, is applied to the motors of the robot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24261f3",
   "metadata": {},
   "source": [
    "### 2.5 Motion control\n",
    "<p>$\\;\\;\\;\\;\\;\\;$ Thymio has a constant speed but we need motion control to ensure smooth turns and to correct the drifting or the wheels' error. A controller seems to be the best solution to these problematics.\n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ Therefore, we chose to use a proportional-derivative (PD) controller to calculate our control speed, that will influence the wheel speeds in order to orientate the robot towards its current goal. This speed is added to our constant speed (actually, the control speed is added to one of the wheel's speed and subtracted to other one) in a way that the robot never stops to turn on itself. First, our idea was to use a simple proportional controller but we expected some trouble with the local navigation since Thymio has no lateral proximity sensor. Indeed, when the robot doesn't detect the local obstacle anymore, it might suddenly turn and hit the obstacle. Adding a derivative parameter allows us to adjust even more the smoothness of the curves. \n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ The PD controller formula is \n",
    "<br> <h1><center>${v}_{control} = {K}_{p} \\cdot {e}_{angle} + {K}_{d} \\cdot {\\Delta}_{error}$</center></h1> \n",
    "<br> where ${e}_{angle}$ is the error between Thymio's current angle and the angle he is supposed to have to reach its destination. ${\\Delta}_{error}$ stands for the error variation between the previous state and the current one. We empirically found the proportional (Kp) and derivative (Kd) gains : ${K}_{p}$ = 12 and ${K}_{d}$ = 6.\n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$Now that we have our control speed and our avoidance speed, defined in section 2.4, with just need to add them to our basic speed (100) that keeps the robot in movement while the final goal isn't reached. Theses speeds were calculated in a way that we could add their value to the left motor speed and substract it to the right one. The following equation shows this sum that is then applied to the motors :\n",
    "<br> <h1><center>${v}_{left} = {v}_{basic} + {v}_{control} \\cdot {s}_{control} + {v}_{avoidance} \\cdot {s}_{avoidance}$</center></h1>\n",
    "<br> <h1><center>${v}_{left} = {v}_{basic} - {v}_{control} \\cdot {s}_{control} - {v}_{avoidance} \\cdot {s}_{avoidance}$</center></h1>\n",
    "<br>where ${s}_{control}$ and ${s}_{avoidance}$ are the scales that ensure that one speed doesn't overtakes the others and that the movement of the robot stays smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cd715c",
   "metadata": {},
   "source": [
    "## 3. Global Code Execution\n",
    "<p>$\\;\\;\\;\\;\\;\\;$ First, here is a graph that shows the general architecture of our executable code :\n",
    "<img src=\"Images/architecture.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d7d84",
   "metadata": {},
   "source": [
    "### 3.1 Imports\n",
    "<p>At first, we will start by importing the different .py files that we created and where are defined most of the functions we will use in this program :</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b44a6",
   "metadata": {},
   "source": [
    "- **init_map.py** defines the functions related to a linear Kalman filter\n",
    "- **motion_control.py** defines the functions related to the motion control of the robot (PD controller, local avoidance)\n",
    "- **vis_graph.py** defines the functions related to the creation of the map and the visibility graph\n",
    "- **update_pos.py** defines the functions that are used to detect the position and orientation of the robot with the camera\n",
    "- **kalman_filter.py** defines the functions used to apply our Kalman filter to the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()\n",
    "from tdmclient import ClientAsync, aw\n",
    "client = ClientAsync()\n",
    "client.process_waiting_messages()\n",
    "node = await client.wait_for_node()\n",
    "aw(node.run())\n",
    "aw(node.stop())\n",
    "aw(node.unlock())\n",
    "\n",
    "import time\n",
    "import cv2\n",
    "from init_map import *\n",
    "from motion_control import *\n",
    "from vis_graph import *\n",
    "from update_pos import *\n",
    "from kalman_filter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80c100",
   "metadata": {},
   "source": [
    "### 3.2 Definition of getters and setters\n",
    "<p>In the following section, we define the getters and setters that will allow us to communicate with the robot regarding the sensors and the motor speeds :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71743c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tdmclient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15512/4011094754.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mtdmclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_var\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_prox_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mprox_horizontal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprox_horizontal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tdmclient' is not defined"
     ]
    }
   ],
   "source": [
    "@tdmclient.notebook.sync_var\n",
    "def get_prox_value():\n",
    "    global prox_horizontal\n",
    "    return prox_horizontal\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def set_motor_speed(left_speed, right_speed):\n",
    "    global motor_left_target, motor_right_target\n",
    "    motor_left_target = left_speed\n",
    "    motor_right_target = right_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894ec32",
   "metadata": {},
   "source": [
    "### 3.3 Initialisation of the variables and constants\n",
    "<p>At first, we define several constants that will be used in the initialisation :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create constant variables\n",
    "next_goal_trigger = 20\n",
    "delta_t = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba006b4c",
   "metadata": {},
   "source": [
    "### 3.4 Creation of the map and the global path\n",
    "<p>In this section, we will explain how the map is created with the camera at the start of the program. The following cell allows to show the shortest path computed on our map.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34947a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VideoCap = cv2.imread(\"Images/Map_init.png\")\n",
    "\n",
    "#Initialisation of the variables that will register the coordinates of the starting point and the final goal\n",
    "start = []\n",
    "stop = []\n",
    "\n",
    "#Creation of the map and the global path\n",
    "while(len(stop)==0 or (len(start)==0)):\n",
    "    start, stop = detect_start_stop(VideoCap)\n",
    "\n",
    "global_path = initialisation(VideoCap, start, stop)\n",
    "start = global_path[0]\n",
    "stop = global_path[len(global_path)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76229037",
   "metadata": {},
   "source": [
    "### 3.5 Initialisation of the variables\n",
    "<p>In this section, we will initialise the different variables that will be used during the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ffeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial motor speeds at 0\n",
    "set_motor_speed(0, 0)\n",
    "\n",
    "# Initialisation of the sensors\n",
    "prox_sensors = np.array([0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# Initialisation of the Kalman filter\n",
    "kalman_filter = KalmanFilter(delta_t)\n",
    "\n",
    "# Find the initial position and orientation of the robot\n",
    "position, orientation = detect_thymio(start, 0, 0, 0, kalman_filter, frame)\n",
    "\n",
    "# Initialisation of the variables that will keep in memory the position and orientation of the last iteration\n",
    "old_position = position\n",
    "old_orientation = orientation\n",
    "\n",
    "# Initialisation of the variables that will keep track of the current goal position and orientation\n",
    "current_goal = 1\n",
    "current_goal_pos = global_path[current_goal]\n",
    "\n",
    "distance_to_goal, current_goal_orientation = calculation_distance_and_angle(position, current_goal_pos)\n",
    "\n",
    "# Initialisation of the variables that will be updated to set the speed of the wheels\n",
    "left_speed = 0\n",
    "right_speed = 0\n",
    "\n",
    "# Initialisation of the boolean that will recall if the camera detection is possible or not at each iteration\n",
    "possible_detection = True\n",
    "\n",
    "# Initialisation of the variable that will register the number of steps yet to finish an oobstacle avoidance\n",
    "avoiding_steps =  0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a6c5b5",
   "metadata": {},
   "source": [
    "## 3.6 Main loop\n",
    "<p>Finally, we can now start implementing the program on Thymio.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    # Refreshing the camera\n",
    "    ret, frame = VideoCap.read()\n",
    "    current_goal_pos = global_path[current_goal]\n",
    "    \n",
    "    # Get the values of the sensors \n",
    "    prox_sensors = get_prox_value()\n",
    "    \n",
    "    # Calculate the position and orientation of the robot    \n",
    "    position, orientation = detect_thymio(old_position, old_orientation, left_speed, right_speed, kalman_filter, frame)\n",
    "    # Drawing the position and the oriention on the image\n",
    "    cv2.circle(frame, (int(position[0]), int(position[1])), 10, (255, 0, 0), 2)\n",
    "    x2 = position[0] + 50 * np.cos(orientation)\n",
    "    y2 = position[1] + 50 * np.sin(orientation)\n",
    "    cv2.line(frame, (int(position[0]), int(position[1])), (int(x2), int(y2)),(0, 255, 0), 2)\n",
    "    for i in range (len(global_path)-1):\n",
    "        cv2.line(frame, (int(global_path[i][0]), int(global_path[i][1])), (int(global_path[i+1][0]), int(global_path[i+1][1])),(0, 0, 255), 2)\n",
    "    cv2.imshow('image', frame)\n",
    "  \n",
    "    # Update the current goal orientation and the distance from the robot\n",
    "    distance_to_goal, current_goal_orientation = calculation_distance_and_angle(position, current_goal_pos)\n",
    "\n",
    "    # Find the speeds to apply to the robot\n",
    "    left_speed, right_speed, avoiding_steps = motion_control(orientation, old_orientation, current_goal_orientation, prox_sensors, avoiding_steps)\n",
    "    \n",
    "    # Apply the desired speeds to the robot\n",
    "    set_motor_speed(int(left_speed), int(right_speed))\n",
    "    \n",
    "    # Keep in memory the position and orientation of the robot for the next \n",
    "    old_position = position\n",
    "    old_orientation = orientation\n",
    "    \n",
    "    # Update the current goal if necessary\n",
    "    if (distance_to_goal < next_goal_trigger) :\n",
    "        current_goal += 1\n",
    "    \n",
    "    if (current_goal == len(global_path)) :\n",
    "        set_motor_speed(0, 0)\n",
    "        break\n",
    "    if cv2.waitKey(1)&0xFF==ord('q'):\n",
    "        break\n",
    "    await client.sleep(0.05)\n",
    "\n",
    "# Close the window when the robot has reached its goal\n",
    "VideoCap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42606e9e",
   "metadata": {},
   "source": [
    "## 4. References\n",
    "- visibility graph/shortest path : https://github.com/TaipanRex/pyvisgraph/tree/master/pyvisgraph\n",
    "- extended kalman filter : https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/\n",
    "- kalman filter, l42 project : https://github.com/L42Project/Tutoriels/blob/master/Divers/tutoriel36/KalmanFilter.py\n",
    "- kalman filter, wikipedia : https://en.wikipedia.org/wiki/Kalman_filter\n",
    "- obstacle avoidance, TP 3 of the course : http://localhost:8906/notebooks/Ex3/Solutions%20Week%203%20-%20Artificial%20neural%20networks.ipynb\n",
    "- openCV documentation : https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
