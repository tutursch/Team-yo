{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9adf41",
   "metadata": {},
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" width=500></td>\n",
    "<img src=\"https://www.zigobot.ch/images/stories/virtuemart/product/Thymio_II_5288c11c8c241.jpg\" width=500>\n",
    "# <center>Project of Basics of mobile robotics</center>\n",
    "## <center> Broccard Brendan, Ferreira Lopes Filipe, Pillet Maxime, Schramm Arthur</center>\n",
    "<hr style=\"clear:both\">\n",
    "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
    "This Juypter notebook will be the report of our project as part of the course \"Basics of mobile robotics\" given by Prof. Francesco Mondada.</p>\n",
    "<hr style=\"clear:both\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c08e7",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "<p>$\\;\\;\\;\\;\\;\\;$ In this project, the goal was to create an environment in which we would have a starting point, a goal to reach and multiple obstacles in the way. We implented the following code on our Thymio robot in order to make it go from the starting point to the goal, following the most efficient path through the initial obstacles and using its sensors to avoid sudden obstacles on its way. A camera allows us to create a map of the environment at the beginning of the program and further on to detect the robot's position and orientation. That information is first filtered with a Kalman filter and then processed by a PD controller that we created in order to make Thymio go smoothly in direction of it's desired goal. Thanks to the Kalman filter, if the camera is suddenly obstruated, we can still estimate the robot's position and orientation. We update our values in a main \"while\" loop that occurs every 0.1 second.\n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ This is a typical map pattern with the thymio at the start, a local obstacle on its way, the final goal in green and the obstacles in black : \n",
    "    \n",
    "<img src=\"Images/map.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3606e03",
   "metadata": {},
   "source": [
    "## 2. Details on each section of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c2fd8",
   "metadata": {},
   "source": [
    "### 2.1 Vision\n",
    "$\\;\\;\\;\\;\\;\\;$ In this section, we will explain how we implemented the vision to our project. The goal is to understand the map and get the positions of the start/stop points and the corners of our obstacles. \n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ The first step of the vision is a \"while\" loop at the beginning of our process, even before the Thymio is placed on the map. The loop is asked to detect at least the start and stop chips which is the condition to stop executing the loop. The chips are differientiated by their HSV color code, the start is blue and the finish green. It also checks if we have initial obstacles in our circuit and computes the positions of their corners. We consider the corners shifted in order to keep a security margin that considers Thymio's width, which results in mapping the obstacles wider than they really are to ensure their complete avoidance.\n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ This first vision step gives us a map with our start, stop and enlarged corners positions : \n",
    "\n",
    "<img src=\"Images/Vision_avec_legende.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ The second step is to detect the position and the facing angle of Thymio after we placed it on the start chip. It is continuously computed since it is updated in our main loop. To do so, we pasted a yellow chip in the front of Thymio and a red one in the back. The robot's position is considered as being in the middle of the two chips. The position of the yellow chip, coupled with the position of the red one, allows us to have the orientation of Thymio. \n",
    "<br>\n",
    "<br> $\\;\\;\\;\\;\\;\\;$The robot's position is given by the blue circle and its orientation is given by the green line :\n",
    "<br>\n",
    "<img src=\"Images/Thymio_orientation.png\" alt=\"Drawing\" style=\"width: 150px;\"/>\n",
    "    \n",
    "$\\;\\;\\;\\;\\;\\;$ We can see in the following image, a mask that shows the stop detection. During the project, we sometimes had to modify the HSV values to detect the start/stop, because it is really sensitive to brightness variations.\n",
    "<img src=\"Images/stop_detection.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6898bf5e",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$Finally, as an example, you can run the following cell to see, with our specific set up, what map and local goals (encircled) we obtained :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d88a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from init_map import *\n",
    "\n",
    "VideoCap = cv2.imread(\"Images/Map_init.png\")\n",
    "\n",
    "#Initialisation of the variables that will register the coordinates of the starting point and the final goal\n",
    "start = []\n",
    "stop = []\n",
    "\n",
    "#Creation of the map and the global path\n",
    "while(len(stop)==0 or (len(start)==0)):\n",
    "    start, stop = detect_start_stop(VideoCap)\n",
    "\n",
    "global_path = initialisation(VideoCap, start, stop)\n",
    "start = global_path[0]\n",
    "stop = global_path[len(global_path)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef2e0c",
   "metadata": {},
   "source": [
    "### 2.2 Global Navigation\n",
    "<p>$\\;\\;\\;\\;\\;\\;$ The global navigation computes the shortest path to go from a starting point to a finish point while going through our obstacle field. We chose to use the visibility graph as the pathfinding algorithm. It consists of a web of every possible routes to go from a point A to a point B passing by the angles of obstacles. Next, it calculates the shortest euclidian route and considers it as the optimized path to take. This path is given in the form of an array of points to reach in the given order.  \n",
    "<br> \n",
    "<br>$\\;\\;\\;\\;\\;\\;$ The visibility graph, when applied to our map, returns this path :\n",
    "<img src=\"Images/VisGraph.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5a48cd",
   "metadata": {},
   "source": [
    "### 2.3 Filtering\n",
    "<p>$\\;\\;\\;\\;\\;\\;$ Since the camera's datas are uncertain (due to noise or lack of precision), we use the Kalman filter based on theses datas in order to get a good estimation of the position and orientation of the robot (which are the states of Thymio that we calculate at each time step). In order to estimate the new state, we use the last position, oriention and speed of the motors (linear and rotation). Then, Thymio's states are updated, based on the last measurements, only if the camera detects the robot. This update compares the real measurements and the estimated ones, in order to make the algorithm better (by calculating the residual covariance matrix and the Kalman gain). If the camera doesn't detect Thymio, we use the prediction the localize the robot, that allows the robot to work even though the camera is hidden. The following formula represents the state space model that we use at each time step :\n",
    "<img src=\"Images/Kalman.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ffe3a",
   "metadata": {},
   "source": [
    "### 2.4 Local Navigation\n",
    "<p>$\\;\\;\\;\\;\\;\\;$For this section, most of the code was inspired by the exercise 3 of the course given by Prof. Francesco Mondada. Indeed, in the motion_control.py file, we can find two founctions that are used for local avoidance : check_obstacle and local_avoidance. The first one just returns a boolean that tells us if any of the front sensors has a value that exceeds the trigger value in which case we consider that an obstacle has been detected. If so, the second function will use a system of weights inspired from the exercise mentionned above to return a certain speed value, that will be added to the basic speed of the robot, in order to make it turn smoothly and avoid the obstacle. There will be more info in the following section on motion control concerning how this speed value, that is returned by the function local_avoidance, is applied to the motors of the robot.\n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$It is also important to mention that at first, the robot would avoid the obstacle but once the front of Thymio had passed it, it would turn right back into it because there are no lateral sensors. A solution we found for that is to add a period of 20 iterations (found empirically with tests) during which the robot would take the control speed (defined in section 2.5 below) in account. That means that whenever the robot detects an local obstacle, it enters a sort of avoidance state out of which it is released only if Thymio's front sensors don't exceed the trigger value for 20 iterations. This state is defined by a variable called avoiding_steps that indicated the remaining iterations with no local obstacle detected left to go through before this state is canceled. We have found that it resolves our problem very accurately.\n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$In order to demonstrate this process, you can run the next cell. Indeed, we will give realistic sensor values to the two functions responsible for local avoidance and see that they recall respectively a boolean that indicates if a local obstacle was detected and an avoidance speed value that we would then use in the function motion_control, defined in the next section. In this example, we will give a high value to the fifth sensor, which correspond to the front far right sensor, implying that the robot should slightly deviated to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a1da2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obstacle_detected_example = True\n",
      "avoidance_speed_example = -800\n"
     ]
    }
   ],
   "source": [
    "from motion_control import *\n",
    "\n",
    "obstacle_detected_example = False\n",
    "avoidance_speed_example = 0\n",
    "\n",
    "sensors_example = [0, 0, 0, 0, 1000, 0, 0]\n",
    "\n",
    "obstacle_detected_example = check_obstacle(sensors_example)\n",
    "print(\"obstacle_detected_example =\", obstacle_detected_example)\n",
    "\n",
    "avoidance_speed_example = local_avoidance(sensors_example)\n",
    "print(\"avoidance_speed_example =\", avoidance_speed_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24261f3",
   "metadata": {},
   "source": [
    "### 2.5 Motion control\n",
    "<p>$\\;\\;\\;\\;\\;\\;$ Thymio has a constant speed but we need motion control to ensure smooth turns and to correct the drifting or the wheels' error. A controller seems to be the best solution to these problematics.\n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ Therefore, we chose to use a proportional-derivative (PD) controller to calculate our control speed, that will influence the wheel speeds in order to orientate the robot towards its current goal. This speed is added to our constant speed (actually, the control speed is added to one of the wheel's speed and subtracted to other one) in a way that the robot never stops to turn on itself. First, our idea was to use a simple proportional controller but we expected some trouble with the local navigation since Thymio has no lateral proximity sensor. Indeed, when the robot doesn't detect the local obstacle anymore, it might suddenly turn and hit the obstacle. Adding a derivative parameter allows us to adjust even more the smoothness of the curves. \n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ The PD controller formula is \n",
    "<br> <h1><center>${v}_{control} = {K}_{p} \\cdot {e}_{angle} + {K}_{d} \\cdot {\\Delta}_{error}$</center></h1> \n",
    "<br> where ${e}_{angle}$ is the error between Thymio's current angle and the angle he is supposed to have to reach its destination. ${\\Delta}_{error}$ stands for the error variation between the previous state and the current one. We empirically found the proportional (Kp) and derivative (Kd) gains : ${K}_{p}$ = 12 and ${K}_{d}$ = 6.\n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ Here we have a scheme of a situation where Thymio's angle is ${\\alpha}$ and its aim angle is ${\\beta}$. \n",
    "<br>\n",
    "<img src=\"Images/Team-yo-scheme.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$Now that we have our control speed and our avoidance speed, defined in section 2.4, with just need to add them to our basic speed (150) that keeps the robot in movement while the final goal isn't reached. Theses speeds were calculated in a way that we could add their value to the left motor speed and substract it to the right one. The following equation shows this sum that is then applied to the motors :\n",
    "<br> <h1><center>${v}_{left} = {v}_{basic} + {v}_{control} \\cdot {s}_{control} + {v}_{avoidance} \\cdot {s}_{avoidance}$</center></h1>\n",
    "<br> <h1><center>${v}_{left} = {v}_{basic} - {v}_{control} \\cdot {s}_{control} - {v}_{avoidance} \\cdot {s}_{avoidance}$</center></h1>\n",
    "<br>where ${s}_{control}$ and ${s}_{avoidance}$ are the scales that ensure that one speed doesn't overtakes the others and that the movement of the robot stays smooth.\n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ Let's now illustrate these calculations with some code. Let's take in account that the sensors values are set to 0, otherwise the control speed would not be considered in the sum. We will now set a realistic orientation and old orientation value for the robot, a possible orientation of the goal from the robot and see how the PD controller will create a realistic control speed and how the main motion control function will calculate suitable left and right wheel speeds to be applied to the robot. In our case, we set the robot to be oriented 45° to the right from the goal direction. Thymio should therefore have a higher right motor speed in order for the robot to turn left. Run the cell below and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b50e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_speed_example = -7\n",
      "left_speed_example = basic_speed + control_speed_example*control_scale + avoidance_speed_example*avoidance_scale\n",
      "left_speed_example = 150 + ( -70 ) + 0\n",
      "left_speed_example = 80\n",
      "right_speed_example = basic_speed - control_speed_example*control_scale - avoidance_speed_example*avoidance_scale\n",
      "right_speed_example = 150 - ( -70 ) - 0\n",
      "right_speed_example = 220\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "orientation_example = np.pi/4\n",
    "old_orientation_example = np.pi/4 + np.pi/8\n",
    "goal_orientation_example = 0\n",
    "sensors_example = [0, 0, 0, 0, 0, 0, 0]\n",
    "avoiding_steps_example = 0\n",
    "left_speed_example, right_speed_example = 0, 0\n",
    "\n",
    "control_scale_example = 10\n",
    "basic_speed_example = 150\n",
    "\n",
    "control_speed_example = 0\n",
    "control_speed_example = int(pd_controller(orientation_example, old_orientation_example, goal_orientation_example))\n",
    "print(\"control_speed_example =\", control_speed_example)\n",
    "\n",
    "left_speed_example, right_speed_example, avoiding_steps_example = motion_control(orientation_example, old_orientation_example, goal_orientation_example, sensors_example, avoiding_steps_example)\n",
    "print(\"left_speed_example = basic_speed + control_speed_example*control_scale + avoidance_speed_example*avoidance_scale\")\n",
    "print(\"left_speed_example =\", basic_speed_example, \"+ (\", control_speed_example*control_scale_example, \") + 0\")\n",
    "print(\"left_speed_example =\", int(left_speed_example)+1)\n",
    "\n",
    "print(\"right_speed_example = basic_speed - control_speed_example*control_scale - avoidance_speed_example*avoidance_scale\")\n",
    "print(\"right_speed_example =\", basic_speed_example, \"- (\", control_speed_example*control_scale_example, \") - 0\")\n",
    "print(\"right_speed_example =\", int(right_speed_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc62bb",
   "metadata": {},
   "source": [
    "We can see in this example that the right motor speed will be set at a higher value than the left one, as intended. Therefore, the PD controller and the motion control main function seem to be working fine. Note that the control speed value that is printed is the value before we multiply it by the control scale in the main motion control function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cd715c",
   "metadata": {},
   "source": [
    "## 3. Global Code Execution\n",
    "$\\;\\;\\;\\;\\;\\;$In this part, you will find the complete code that we executed for the presentation and in order to get the results you can find in section 4. However, it is necessary to have our specific set up to run the whole code.\n",
    "<br>\n",
    "<br>$\\;\\;\\;\\;\\;\\;$ First, here is a graph that shows the general architecture of our executable code :\n",
    "<img src=\"Images/architecture.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d7d84",
   "metadata": {},
   "source": [
    "### 3.1 Imports\n",
    "<p>At first, we will start by importing the different .py files that we created and where are defined most of the functions we will use in this program :</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b44a6",
   "metadata": {},
   "source": [
    "- **init_map.py** defines the functions related to a linear Kalman filter\n",
    "- **motion_control.py** defines the functions related to the motion control of the robot (PD controller, local avoidance)\n",
    "- **vis_graph.py** defines the functions related to the creation of the map and the visibility graph\n",
    "- **update_pos.py** defines the functions that are used to detect the position and orientation of the robot with the camera\n",
    "- **kalman_filter.py** defines the functions used to apply our Kalman filter to the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3b3b1",
   "metadata": {},
   "source": [
    "Imports related to Thymio (needs to be connected) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()\n",
    "from tdmclient import ClientAsync, aw\n",
    "client = ClientAsync()\n",
    "client.process_waiting_messages()\n",
    "node = await client.wait_for_node()\n",
    "aw(node.run())\n",
    "aw(node.stop())\n",
    "aw(node.unlock())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f13e4c0",
   "metadata": {},
   "source": [
    "Other imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930fc9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "from init_map import *\n",
    "from motion_control import *\n",
    "from vis_graph import *\n",
    "from update_pos import *\n",
    "from kalman_filter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80c100",
   "metadata": {},
   "source": [
    "### 3.2 Definition of getters and setters\n",
    "<p>In the following section, we define the getters and setters that will allow us to communicate with the robot regarding the sensors and the motor speeds :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c71743c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tdmclient.notebook.sync_var\n",
    "def get_prox_value():\n",
    "    global prox_horizontal\n",
    "    return prox_horizontal\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def set_motor_speed(left_speed, right_speed):\n",
    "    global motor_left_target, motor_right_target\n",
    "    motor_left_target = left_speed\n",
    "    motor_right_target = right_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894ec32",
   "metadata": {},
   "source": [
    "### 3.3 Initialisation of the variables and constants\n",
    "<p>At first, we define several constants that will be used in the initialisation :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7733353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create constant variables\n",
    "next_goal_trigger = 20\n",
    "delta_t = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba006b4c",
   "metadata": {},
   "source": [
    "### 3.4 Creation of the map and the global path\n",
    "<p>In this section, we will explain how the map is created with the camera at the start of the program. The following cell allows to show the shortest path computed on our map.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34947a8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.4) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9776/2064359299.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#Creation of the map and the global path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_start_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVideoCap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mglobal_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialisation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVideoCap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Team-yo\\init_map.py\u001b[0m in \u001b[0;36mdetect_start_stop\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mlo_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolor_stop\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mhi_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolor_stop\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2HSV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblur\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mmask_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minRange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlo_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhi_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.4) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "# Reading the camera\n",
    "VideoCap = cv2.VideoCapture(0)\n",
    "ret, frame = VideoCap.read()\n",
    "\n",
    "#Initialisation of the variables that will register the coordinates of the starting point and the final goal\n",
    "start = []\n",
    "stop = []\n",
    "\n",
    "#Creation of the map and the global path\n",
    "while(len(stop)==0 or (len(start)==0)):\n",
    "    start, stop = detect_start_stop(VideoCap)\n",
    "\n",
    "global_path = initialisation(VideoCap, start, stop)\n",
    "start = global_path[0]\n",
    "stop = global_path[len(global_path)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76229037",
   "metadata": {},
   "source": [
    "### 3.5 Initialisation of the variables\n",
    "<p>In this section, we will initialise the different variables that will be used during the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b21ffeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial motor speeds at 0\n",
    "set_motor_speed(0, 0)\n",
    "\n",
    "# Initialisation of the sensors\n",
    "prox_sensors = np.array([0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# Initialisation of the Kalman filter\n",
    "kalman_filter = KalmanFilter(delta_t)\n",
    "\n",
    "# Find the initial position and orientation of the robot\n",
    "position, orientation = detect_thymio(start, 0, 0, 0, kalman_filter, frame)\n",
    "\n",
    "# Initialisation of the variables that will keep in memory the position and orientation of the last iteration\n",
    "old_position = position\n",
    "old_orientation = orientation\n",
    "\n",
    "# Initialisation of the variables that will keep track of the current goal position and orientation\n",
    "current_goal = 1\n",
    "current_goal_pos = global_path[current_goal]\n",
    "\n",
    "distance_to_goal, current_goal_orientation = calculation_distance_and_angle(position, current_goal_pos)\n",
    "\n",
    "# Initialisation of the variables that will be updated to set the speed of the wheels\n",
    "left_speed = 0\n",
    "right_speed = 0\n",
    "\n",
    "# Initialisation of the boolean that will recall if the camera detection is possible or not at each iteration\n",
    "possible_detection = True\n",
    "\n",
    "# Initialisation of the variable that will register the number of steps yet to finish an oobstacle avoidance\n",
    "avoiding_steps =  0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a6c5b5",
   "metadata": {},
   "source": [
    "### 3.6 Main loop\n",
    "<p>Finally, we can now start implementing the program on Thymio.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05cb22dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8656/2996458910.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Refreshing the camera\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVideoCap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mcurrent_goal_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobal_path\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_goal\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    # Refreshing the camera\n",
    "    ret, frame = VideoCap.read()\n",
    "    \n",
    "    # Updating the position of the current goal\n",
    "    current_goal_pos = global_path[current_goal]\n",
    "    \n",
    "    # Get the values of the sensors \n",
    "    prox_sensors = get_prox_value()\n",
    "    \n",
    "    # Calculate the position and orientation of the robot    \n",
    "    position, orientation = detect_thymio(old_position, old_orientation, left_speed, right_speed, kalman_filter, frame)\n",
    "    # Drawing the position and the oriention on the image\n",
    "    cv2.circle(frame, (int(position[0]), int(position[1])), 10, (255, 0, 0), 2)\n",
    "    x2 = position[0] + 50 * np.cos(orientation)\n",
    "    y2 = position[1] + 50 * np.sin(orientation)\n",
    "    cv2.line(frame, (int(position[0]), int(position[1])), (int(x2), int(y2)),(0, 255, 0), 2)\n",
    "    for i in range (len(global_path)-1):\n",
    "        cv2.line(frame, (int(global_path[i][0]), int(global_path[i][1])), (int(global_path[i+1][0]), int(global_path[i+1][1])),(0, 0, 255), 2)\n",
    "    cv2.imshow('image', frame)\n",
    "  \n",
    "    # Update the current goal orientation and the distance from the robot\n",
    "    distance_to_goal, current_goal_orientation = calculation_distance_and_angle(position, current_goal_pos)\n",
    "\n",
    "    # Find the speeds to apply to the robot\n",
    "    left_speed, right_speed, avoiding_steps = motion_control(orientation, old_orientation, current_goal_orientation, prox_sensors, avoiding_steps)\n",
    "    \n",
    "    # Apply the desired speeds to the robot\n",
    "    set_motor_speed(int(left_speed), int(right_speed))\n",
    "    \n",
    "    # Keep in memory the position and orientation of the robot for the next \n",
    "    old_position = position\n",
    "    old_orientation = orientation\n",
    "    \n",
    "    # Update the current goal if necessary\n",
    "    if (distance_to_goal < next_goal_trigger) :\n",
    "        current_goal += 1\n",
    "    \n",
    "    if (current_goal == len(global_path)) :\n",
    "        set_motor_speed(0, 0)\n",
    "        break\n",
    "    if cv2.waitKey(1)&0xFF==ord('q'):\n",
    "        break\n",
    "    await client.sleep(0.05)\n",
    "\n",
    "# Close the window when the robot has reached its goal\n",
    "VideoCap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baba4c4",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "We filmed our Thymio in action and edited a video where we can also see what happens on the control screen. At first, the program shows us the map it created and the global path that will be followed by the robot. The red circles are the temporary goals Thymio will reach until her gets to the end of the path. We can also see that an unexpected obstacle was added after the map was created. As it is on Thymio's path, we can see him use his sensors to locally avoid it. While the robot reaches the second turn, we can see that we obstruated the camera and that Thymio has to rely on the Kalman filter to estimate his position and the result is satisfying as we reveal the environment again during the third turn and that his position was relatively precisely estimated. Finally, the robot reaches the ending point and stop his motors as the program is over. By running the following cell, you can observe all the elements mentionned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e28400c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"450\"\n",
       "            src=\"Images/results.mp4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x21561bd8b80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(width=\"800\", height=\"450\", src=\"Images/results.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42606e9e",
   "metadata": {},
   "source": [
    "## 5. References\n",
    "- visibility graph/shortest path : https://github.com/TaipanRex/pyvisgraph/tree/master/pyvisgraph\n",
    "- extended kalman filter : https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/\n",
    "- kalman filter, l42 project : https://github.com/L42Project/Tutoriels/blob/master/Divers/tutoriel36/KalmanFilter.py\n",
    "- kalman filter, wikipedia : https://en.wikipedia.org/wiki/Kalman_filter\n",
    "- color detection, l42 project : https://github.com/L42Project/Tutoriels/tree/master/OpenCV/tutoriel4\n",
    "- obstacle avoidance, TP 3 of the course : http://localhost:8906/notebooks/Ex3/Solutions%20Week%203%20-%20Artificial%20neural%20networks.ipynb\n",
    "- openCV documentation : https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
